{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot Recommender Laptop with LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-efYJpFdltO"
      },
      "source": [
        "#Natural Language Processing Chatbot Recommender Laptop with LSTM\n",
        "\n",
        "## Steven Tjayadi / 535180085\n",
        "\n",
        "###1. Import Library NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__v6VeyjF0TN",
        "outputId": "52dc2bb9-f037-4f86-bafd-dbd301d15fc0"
      },
      "source": [
        "!pip install virtualenv # Take Requirement for Library list in heroku\n",
        "!pip freeze > requirement.txt\n",
        "!pip install -r requirement.txt"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting virtualenv\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/08/f819421002e85a71d58368f7bffbe0b1921325e0e8ca7857cb5fb0e1f7c1/virtualenv-20.4.7-py2.py3-none-any.whl (7.2MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2MB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from virtualenv) (4.0.1)\n",
            "Requirement already satisfied: appdirs<2,>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (1.4.4)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/26/f6a23dd3e578132cf924e0dd5d4e055af0cd4ab43e2a9f10b7568bfb39d9/distlib-0.3.2-py2.py3-none-any.whl (338kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 38.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (3.0.12)\n",
            "Requirement already satisfied: six<2,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->virtualenv) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->virtualenv) (3.4.1)\n",
            "Installing collected packages: distlib, virtualenv\n",
            "Successfully installed distlib-0.3.2 virtualenv-20.4.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmzvT4bEGRyd",
        "outputId": "0b16069e-7336-4138-8272-2f3dfbfee1f5"
      },
      "source": [
        "!pip install gunicorn # For deploying to heroku"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gunicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/dd/5b190393e6066286773a67dfcc2f9492058e9b57c4867a95f1ba5caf0a83/gunicorn-20.1.0-py3-none-any.whl (79kB)\n",
            "\r\u001b[K     |████▏                           | 10kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 20kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 30kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 40kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 71kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn) (57.0.0)\n",
            "Installing collected packages: gunicorn\n",
            "Successfully installed gunicorn-20.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCyFoN5wd-jO",
        "outputId": "fd498db8-300f-4b49-ce99-e3eedc927209"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Download ini di google colab / Jupyter notebook untuk package tambahan\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQjQ8z0ieJF9"
      },
      "source": [
        "### 2. Import Library tambahan untuk menyimpan file seperti pandas , numpy , json dll"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhACe1K1eIhm"
      },
      "source": [
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx8IC2dgeZ4P"
      },
      "source": [
        "### 3. Import Library Machine Learning dan Deep Learning \n",
        "* Sklearn ( Machine Learning )\n",
        "* Keras ( Deep Learning )\n",
        "* tensorflow ( model deployment + integrated keras )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiJhcmvneaIu"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import keras\n",
        "from keras.models import Sequential , load_model\n",
        "from keras.layers import Dense, Activation, Dropout , LSTM , Embedding\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMU3PayefA9d"
      },
      "source": [
        "### 4. Melakukan import library Dataset Laptop 2 dan Dataset Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgc04SBee4Zm"
      },
      "source": [
        "data_file = open('Dataset Chatbot Laptop.json').read()\n",
        "intents = json.loads(data_file)\n",
        "spek = pd.read_csv(\"Dataset Laptop 2.csv\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTlY5ASlfRPu"
      },
      "source": [
        "### 5. Membuat list baru untuk menyimpan data kata , kelas dan dokumen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3c7y0KefQk-"
      },
      "source": [
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQtziv3aflV1"
      },
      "source": [
        "### 6. Melakukan looping dari json untuk melakukan preprocessing text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hANaK5zMflIm"
      },
      "source": [
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "\n",
        "        #tokenize each word\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)\n",
        "        #add documents in the corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "# lemmaztize and lower each word and remove duplicates\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "pickle.dump(words,open('testing_kata.pkl','wb'))\n",
        "pickle.dump(classes,open('testing.pkl','wb'))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkCAZkRwgF7m"
      },
      "source": [
        "### 7. Melakukan training dan membuat bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coKFG_GOgFtX"
      },
      "source": [
        "training = []\n",
        "# create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # lemmatize each word - create base word, in attempt to represent related words\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "    # create our bag of words array with 1, if word match found in current pattern\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "    \n",
        "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    \n",
        "    training.append([bag, output_row])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd2FtjcsgXmN"
      },
      "source": [
        "### 8. Melakukan pemisahkan antara training X dan Y "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rCxUIyqgXM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ed7707c-ff13-4f72-d95e-8d6d0298ecda"
      },
      "source": [
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "# create train and test lists. X - patterns, Y - intents\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "\n",
        "train_x_convert = np.array(train_x)\n",
        "train_y_convert = np.array(train_y)\n",
        "\n",
        "print(\"Training data created\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data created\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3JNipcfgqn1"
      },
      "source": [
        "### 9. Melakukan training data dengan Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYMmNtKsgqDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a9576d8-3c90-4cc7-fac6-68c852f4f460"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128 , input_shape=(len(train_x[0]),) , activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y_convert[0]), activation='softmax'))\n",
        "\n",
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model \n",
        "hist = model.fit(train_x_convert , train_y_convert , epochs=100, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5', hist)\n",
        "\n",
        "print(\"model created\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 1s 2ms/step - loss: 2.9787 - accuracy: 0.0559\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.9442 - accuracy: 0.0491\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.8208 - accuracy: 0.1489\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.8127 - accuracy: 0.0638\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 2.7508 - accuracy: 0.1861\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.6734 - accuracy: 0.1207\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.6491 - accuracy: 0.1610\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.6168 - accuracy: 0.2274\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.5190 - accuracy: 0.2586\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 2.5387 - accuracy: 0.2591\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.3092 - accuracy: 0.4386\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.3098 - accuracy: 0.3129\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 2.0329 - accuracy: 0.4577\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.9922 - accuracy: 0.3243\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 2.2461 - accuracy: 0.2588\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.9988 - accuracy: 0.4170\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.8902 - accuracy: 0.4149\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.8143 - accuracy: 0.4062\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.6185 - accuracy: 0.5284\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.5982 - accuracy: 0.5170\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.6837 - accuracy: 0.3985\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.5564 - accuracy: 0.5375\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.2160 - accuracy: 0.5169\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.1981 - accuracy: 0.6744\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.2191 - accuracy: 0.6157\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.0764 - accuracy: 0.6539\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.4079 - accuracy: 0.5349\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.1150 - accuracy: 0.6802\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.0184 - accuracy: 0.6490\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.0093 - accuracy: 0.6919\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.8966 - accuracy: 0.7468\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.2291 - accuracy: 0.6437\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.7668 - accuracy: 0.6988\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.9215 - accuracy: 0.6871\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.8911 - accuracy: 0.6566\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.9169 - accuracy: 0.6537\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.7056 - accuracy: 0.7342\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.9825 - accuracy: 0.5975\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.8070 - accuracy: 0.7024\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.8363 - accuracy: 0.7055\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.7599 - accuracy: 0.7395\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.6505 - accuracy: 0.7804\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.7185 - accuracy: 0.7431\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6345 - accuracy: 0.8555\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.5635 - accuracy: 0.8105\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5951 - accuracy: 0.8088\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.6489 - accuracy: 0.7351\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.6188 - accuracy: 0.8069\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.5356 - accuracy: 0.8227\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5156 - accuracy: 0.8167\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.7856 - accuracy: 0.7327\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.5795 - accuracy: 0.7954\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.5315 - accuracy: 0.7640\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7989\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.7594\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5648 - accuracy: 0.8040\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5211 - accuracy: 0.8523\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.6351 - accuracy: 0.7401\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4729 - accuracy: 0.8285\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5644 - accuracy: 0.8047\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5241 - accuracy: 0.7479\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.4657 - accuracy: 0.8346\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.6179 - accuracy: 0.7723\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.6032 - accuracy: 0.7479\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4824 - accuracy: 0.8277\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6229 - accuracy: 0.8110\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3985 - accuracy: 0.8196\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.4325 - accuracy: 0.8439\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.5293 - accuracy: 0.7972\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.4616 - accuracy: 0.8497\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4505 - accuracy: 0.8501\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.6070 - accuracy: 0.8048\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.5814 - accuracy: 0.7836\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.8146\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5223 - accuracy: 0.7833\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.3663 - accuracy: 0.8399\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4751 - accuracy: 0.8601\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5970 - accuracy: 0.7320\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4436 - accuracy: 0.8193\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.4091 - accuracy: 0.8704\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.3699 - accuracy: 0.8491\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.4444 - accuracy: 0.8186\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.4482 - accuracy: 0.8332\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.4224 - accuracy: 0.7759\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5499 - accuracy: 0.7290\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.4840 - accuracy: 0.8113\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.7891\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4870 - accuracy: 0.7602\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.8144\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.4512 - accuracy: 0.8235\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.8086\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4107 - accuracy: 0.8005\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.3616 - accuracy: 0.8798\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.3697 - accuracy: 0.8520\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.3995 - accuracy: 0.8224\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.4730 - accuracy: 0.7855\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.4681 - accuracy: 0.7607\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4201 - accuracy: 0.8407\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3283 - accuracy: 0.8404\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.3155 - accuracy: 0.8348\n",
            "model created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBY3G-fQg5t2"
      },
      "source": [
        "### 10. Melakukan load model untuk di testing dari hasil traning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os0x9ISog5Ge"
      },
      "source": [
        "model = load_model('chatbot_model.h5')\n",
        "intents = json.loads(open('Dataset Chatbot Laptop.json').read())\n",
        "words = pickle.load(open('testing_kata.pkl','rb'))\n",
        "classes = pickle.load(open('testing.pkl','rb'))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvVck3dthZkb"
      },
      "source": [
        "### 11. Membuat Function Predict untuk melakukan predict pada Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8OsAJQlheNr"
      },
      "source": [
        "#### 1. Melakukan Cleaning kalimat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUhSfRgThZKs"
      },
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # tokenize the pattern - split words into array\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stem each word - create short form for word\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8Quve3WhplG"
      },
      "source": [
        "#### 2. Melakukan Bag of words untuk predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVXntm-UhpaM"
      },
      "source": [
        "def bow(sentence, words, show_details=True):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # bag of words - matrix of N words, vocabulary matrix\n",
        "    bag = [0]*len(words)  \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                # assign 1 if current word is in the vocabulary position\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "    return(np.array(bag))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtpdXjuuhzT9"
      },
      "source": [
        "#### 3. Melakukan Predict sesuai kelas kategori ( Classes )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfdXzLm8hzH4"
      },
      "source": [
        "def predict_class(sentence, model):\n",
        "    # filter out predictions below a threshold\n",
        "    p = bow(sentence, words,show_details=False)\n",
        "    res = model.predict(np.array([p]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "    return return_list"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72qlEou_h9AN"
      },
      "source": [
        "#### 4. Memanggil Response dalam Chatbot "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2QuH97mQtfw"
      },
      "source": [
        "def getResponse(ints, intents_json):\n",
        "    tag = ints[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    for i in list_of_intents:\n",
        "        if(i['tag']== tag):\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "    return result\n",
        "\n",
        "def chatbot_response(msg):\n",
        "    ints = predict_class(msg, model)\n",
        "    res = getResponse(ints, intents)\n",
        "    return res"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk7a0VMLiTbI"
      },
      "source": [
        "### 12. Melakukan Deployment ke website menggunakan Flask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCjWnqnPiam-"
      },
      "source": [
        "#### 1. Melakukan instalasi flask-ngrok ( Jika di google colab )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLEJkNp4mVor",
        "outputId": "d5d547c3-6c80-4c65-fa17-88f76d6f08c6"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.7/dist-packages (0.0.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4QMazG2igkt"
      },
      "source": [
        "#### 2. Deployment hasil model melalui website dari Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB2bUxj8mZ8q",
        "outputId": "f1914ea2-9bd2-4c26-91df-b060de9f1ea3"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask , render_template , request , url_for\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "run_with_ngrok(app)   #starts ngrok when the app is run jika di google colab\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home(): \n",
        "    return render_template(\"Design Chatbot.html\")\n",
        "\n",
        "list_question = []\n",
        "list_answer = []\n",
        "\n",
        "@app.route(\"/get\")\n",
        "def get_bot_response():    \n",
        "    userText = request.args.get('msg')\n",
        "    list_question.append(userText)\n",
        "    list_answer.append(str(chatbot_response(userText)))     \n",
        "    return str(chatbot_response(userText))\n",
        " \n",
        "  \n",
        "if __name__ == \"__main__\":\n",
        "    app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://e310e34f00bc.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwxzruUoAAXA"
      },
      "source": [
        "list_question"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLLUf_JIAJWa"
      },
      "source": [
        "list_answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIcaswA-QY_T"
      },
      "source": [
        "print(\"Chatop\")\n",
        "\n",
        "for i in range(0,1000):\n",
        "    question = input(\"You : \")\n",
        "    print(\"Bot :\" , chatbot_response(question))\n",
        "\n",
        "    if question in (\"See you\" , \"see you\" , \"Bye\"):\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GFdcFKRCTaA"
      },
      "source": [
        "### 13. Training Rekomendasi Laptop berdasarkan hasil input dan output Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmS3ARUoaUQV"
      },
      "source": [
        "dropped_column = [\"Nama Laptop\" , \"Size\" , \"Processor\" , \"GPU\" , \"Storage\" , \"Link\" , \"Harga\" , \"RAM\"] ## List column ambil dari responses chatbot nya\n",
        "X = spek.drop(dropped_column , axis = 1)\n",
        "value_x = X.values\n",
        "Y = spek.iloc[:,0]\n",
        "X\n",
        "\n",
        "## Reshaping untuk Tokenizer text\n",
        "reshaping_x = []\n",
        "\n",
        "for i in range(len(value_x)):\n",
        "    for j in range(len(value_x[i])):\n",
        "        reshaping_x.append(value_x[i][j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9ASVttmC-Ww"
      },
      "source": [
        "### 14. Preprocessing Text dengan tokenizer Deep Learning dari Keras dan Scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQMzZVdxadUF"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(reshaping_x)\n",
        "sequences = tokenizer.texts_to_sequences(reshaping_x)\n",
        "\n",
        "sequences_numpy = np.array(sequences)\n",
        "\n",
        "j = len(sequences_numpy)/len(X)\n",
        "sequences_numpy = sequences_numpy.reshape(8,2)\n",
        "\n",
        "vocabulary_size = len(tokenizer.word_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX7Q-4eGawCu"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder # Mengonvert hasil y menjadi kode angka\n",
        "converter = LabelEncoder()\n",
        "Y_convert = converter.fit_transform(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndk7dQ8kayLN"
      },
      "source": [
        "import keras.utils\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "x = sequences_numpy\n",
        "y = Y_convert\n",
        "\n",
        "y = to_categorical(y , num_classes=vocabulary_size+1)\n",
        "\n",
        "seq_len = len(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C_4NaHoEUVN"
      },
      "source": [
        "### 15. Membuat Model Deep Learning LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GHiEPULap2l"
      },
      "source": [
        "def create_model(vocabulary_size, seq_len):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocabulary_size, 10, input_length=seq_len))\n",
        "    model.add(LSTM(100, return_sequences=True))\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "\n",
        "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "    \n",
        "    # Categorical Cross Entropy = Cost Function Classification di Machine Learning\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "   \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiaHoalHa1q_"
      },
      "source": [
        "# define model\n",
        "model = create_model(vocabulary_size+1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxaOEq4rEaA9"
      },
      "source": [
        "#### Melakukan training LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKCT6Qfra372"
      },
      "source": [
        "# Melakukan training LSTM\n",
        "model.fit(np.array(x), np.array(y), batch_size=128, epochs=75,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTu1qJS6HSp3"
      },
      "source": [
        "### 16. Melakukan Predict Hasil Rekomendasi "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud8xAaZba6uu"
      },
      "source": [
        "predict_value = model.predict(converter.fit_transform(Y))\n",
        "predict_values = predict_value[0]\n",
        "predict_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjM_bh9Fl7jv"
      },
      "source": [
        "make_integer = []\n",
        "for i in range(len(predict_values)):\n",
        "    making_integer = round(predict_values[i])\n",
        "    make_integer.append(making_integer)\n",
        "make_integer\n",
        "\n",
        "what_laptop = converter.inverse_transform(make_integer)\n",
        "print(what_laptop[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Sir0k3UHgTh"
      },
      "source": [
        "#### Menyimpan hasil data untuk hasil list rekomendasi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjyzU4FJ8Chp"
      },
      "source": [
        "Laptop_first_recommended = {\"Nama Laptop\" : [] , \"Brand\" : [] , \"Size\" : [] , \"Processor\" : [] , \"RAM\" : [] , \"GPU\" : [] , \"Storage\" : [] , \"Harga\" : [] , \"Link\" : []} \n",
        "\n",
        "search_justy = spek.iloc[:,0].values\n",
        "\n",
        "for searchlaptop in range(len(search_justy) + 1):\n",
        "    if what_laptop[0] == search_justy[searchlaptop]:\n",
        "        Laptop_first_recommended[\"Nama Laptop\"].append(search_justy[searchlaptop])\n",
        "        Laptop_first_recommended[\"Brand\"].append(spek.iloc[searchlaptop,1])\n",
        "        Laptop_first_recommended[\"Size\"].append(spek.iloc[searchlaptop,2])\n",
        "        Laptop_first_recommended[\"Processor\"].append(spek.iloc[searchlaptop,3])\n",
        "        Laptop_first_recommended[\"RAM\"].append(spek.iloc[searchlaptop,4])\n",
        "        Laptop_first_recommended[\"GPU\"].append(spek.iloc[searchlaptop,5])\n",
        "        Laptop_first_recommended[\"Storage\"].append(spek.iloc[searchlaptop,6])\n",
        "        Laptop_first_recommended[\"Harga\"].append(spek.iloc[searchlaptop,9])\n",
        "        Laptop_first_recommended[\"Link\"].append(spek.iloc[searchlaptop,8])\n",
        "        break\n",
        "\n",
        "data_laptop = pd.DataFrame(Laptop_first_recommended)\n",
        "data_laptop\n",
        "\n",
        "result = data_laptop.to_json(orient=\"records\")\n",
        "parsed = json.loads(result)\n",
        "json.dumps(parsed , indent = 4)\n",
        "with open('Data_rekomendasilaptop.json' , 'w') as f:\n",
        "    json.dump(parsed,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAMB9OFOH9Ax"
      },
      "source": [
        "openfile = json.loads(open('Dataset Chatbot Laptop.json').read())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}